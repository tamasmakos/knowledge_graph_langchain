{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing with the bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading a text file\n",
    "with open('bible.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import json\n",
    "\n",
    "# def preprocess_bible_text(text):\n",
    "#     # Remove the first two lines (CPDV and Catholic Public Domain Version)\n",
    "#     lines = text.split('\\n')[2:]\n",
    "    \n",
    "#     structured_text = []\n",
    "#     current_book = \"\"\n",
    "#     current_chapter = \"\"\n",
    "    \n",
    "#     for line in lines:\n",
    "#         # Use regex to extract book, chapter, verse, and content\n",
    "#         match = re.match(r'(\\w+)\\s+(\\d+):(\\d+)\\s+(.+)', line)\n",
    "#         if match:\n",
    "#             book, chapter, verse, content = match.groups()\n",
    "            \n",
    "#             # If we've encountered a new book, add it to the structure\n",
    "#             if book != current_book:\n",
    "#                 structured_text.append({\"type\": \"Book\", \"name\": book, \"chapters\": []})\n",
    "#                 current_book = book\n",
    "            \n",
    "#             # If we've encountered a new chapter, add it to the current book\n",
    "#             if chapter != current_chapter:\n",
    "#                 structured_text[-1][\"chapters\"].append({\"number\": int(chapter), \"verses\": []})\n",
    "#                 current_chapter = chapter\n",
    "            \n",
    "#             # Ensure structured_text has chapters before accessing\n",
    "#             if structured_text and structured_text[-1][\"chapters\"]:\n",
    "#                 structured_text[-1][\"chapters\"][-1][\"verses\"].append({\n",
    "#                     \"number\": int(verse),\n",
    "#                     \"text\": content\n",
    "#                 })\n",
    "    \n",
    "#     return structured_text\n",
    "\n",
    "# def split_structured_text(structured_text):\n",
    "#     chunks = []\n",
    "#     current_chunk = []\n",
    "#     verse_count = 0\n",
    "\n",
    "#     for book in structured_text:\n",
    "#         for chapter in book[\"chapters\"]:\n",
    "#             for verse in chapter[\"verses\"]:\n",
    "#                 current_chunk.append({\n",
    "#                     \"book\": book[\"name\"],\n",
    "#                     \"chapter\": chapter[\"number\"],\n",
    "#                     \"verse\": verse[\"number\"],\n",
    "#                     \"text\": verse[\"text\"]\n",
    "#                 })\n",
    "#                 chunks.append(current_chunk)\n",
    "#                 current_chunk = []\n",
    "\n",
    "#     if current_chunk:\n",
    "#         chunks.append(current_chunk)\n",
    "\n",
    "#     return chunks\n",
    "\n",
    "# # Preprocess the Bible text\n",
    "# preprocessed_text = preprocess_bible_text(text)\n",
    "\n",
    "# #write to json\n",
    "# with open('bible_structured.json', 'w') as f:\n",
    "#     json.dump(preprocessed_text, f)\n",
    "\n",
    "# #read from json\n",
    "# with open('bible_structured.json', 'r') as f:\n",
    "#     preprocessed_text = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "import streamlit as st\n",
    "\n",
    "NEO4J_URI = st.secrets[\"NEO4J_URI\"]\n",
    "NEO4J_USERNAME = st.secrets[\"NEO4J_USERNAME\"]\n",
    "NEO4J_PASSWORD = st.secrets[\"NEO4J_PASSWORD\"]\n",
    "NEO4J_DATABASE = st.secrets[\"NEO4J_DATABASE\"]\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    database=NEO4J_DATABASE\n",
    ")\n",
    "\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=1,\n",
    "    max_bucket_size=10687,\n",
    ")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    temperature=0.25, \n",
    "    model_name=\"llama3-groq-70b-8192-tool-use-preview\",\n",
    "    rate_limiter=rate_limiter,\n",
    "    api_key=st.secrets[\"GROQ_API_KEY\"]\n",
    ")\n",
    "\n",
    "allowed_nodes = [\n",
    "    \"Person\", \"Concept\", \"Place\", \"Event\"\n",
    "\n",
    "]\n",
    "\n",
    "allowed_relationships = [\n",
    "    \"MENTIONS\",\n",
    "    \"SPEAKS_TO\",\n",
    "    \"IS_LOCATED_IN\",\n",
    "    \"HAPPENS_WITH\",\n",
    "    \"HAPPENS_AT\"\n",
    "\n",
    "]\n",
    "\n",
    "def create_custom_prompt(verse):\n",
    "    return ChatPromptTemplate.from_template(f\"\"\"\n",
    "You are a biblical scholar tasked with extracting detailed information from the Bible to create a comprehensive knowledge graph.\n",
    "Given the following structured Bible text, please identify and extract the following elements:\n",
    "\n",
    "1. People mentioned\n",
    "2. Places mentioned\n",
    "3. Events described\n",
    "4. Key concepts or themes\n",
    "5. Important artifacts or objects\n",
    "6. Ethnic Groups or collective entities\n",
    "8. Relationships between these elements\n",
    "\n",
    "Structured Bible text:\n",
    "{verse}\n",
    "\n",
    "For each element, provide the following information:\n",
    "- Node type (Person, Place, Event, Concept, Book, Chapter, Verse, Artifact, Ethnic Group)\n",
    "- Specific subtype if applicable\n",
    "- Relationships to other nodes, using the expanded relationship types\n",
    "\"\"\")\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(\n",
    "    llm=llm,\n",
    "    allowed_nodes=allowed_nodes,\n",
    "    allowed_relationships=allowed_relationships,\n",
    "    strict_mode=True,\n",
    "    node_properties = True,\n",
    "    relationship_properties = True,\n",
    "    ignore_tool_usage = False\n",
    ")\n",
    "\n",
    "def preprocess_bible_text(text):\n",
    "    lines = text.split('\\n')[2:]\n",
    "    structured_text = []\n",
    "    current_book = \"\"\n",
    "    current_chapter = \"\"\n",
    "    \n",
    "    for line in lines:\n",
    "        match = re.match(r'(\\w+)\\s+(\\d+):(\\d+)\\s+(.+)', line)\n",
    "        if match:\n",
    "            book, chapter, verse, content = match.groups()\n",
    "            if book != current_book:\n",
    "                structured_text.append({\"type\": \"Book\", \"name\": book, \"chapters\": []})\n",
    "                current_book = book\n",
    "            if chapter != current_chapter:\n",
    "                structured_text[-1][\"chapters\"].append({\"number\": int(chapter), \"verses\": []})\n",
    "                current_chapter = chapter\n",
    "            if structured_text and structured_text[-1][\"chapters\"]:\n",
    "                structured_text[-1][\"chapters\"][-1][\"verses\"].append({\n",
    "                    \"number\": int(verse),\n",
    "                    \"text\": content\n",
    "                })\n",
    "    return structured_text\n",
    "\n",
    "preprocessed_text = preprocess_bible_text(text)\n",
    "\n",
    "selected_books = [\"Matthew\", \"Mark\", \"Luke\", \"John\"]\n",
    "filtered_books = []\n",
    "\n",
    "for book in preprocessed_text:\n",
    "    if book[\"name\"] in selected_books:\n",
    "        filtered_books.append(book)\n",
    "\n",
    "verses = []\n",
    "for book in filtered_books:\n",
    "    for keys in book.keys():\n",
    "        if keys == 'chapters':\n",
    "            book_name = book['name']\n",
    "            for chapter in book[keys]:\n",
    "                chapter_name = chapter['number']\n",
    "                verses_in_chapter = []\n",
    "                for verse in chapter['verses']:\n",
    "                    verses_in_chapter.append(verse['text'])\n",
    "                verses.append(verses_in_chapter)\n",
    "\n",
    "paste_verses = []\n",
    "for verse in verses:\n",
    "    verse = ' '.join(verse)\n",
    "    paste_verses.append(verse)\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)\n",
    "wikipedia_tool = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize SerpAPI\n",
    "serpapi = SerpAPIWrapper(serpapi_api_key='')\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=serpapi.run,\n",
    "        description=\"Useful for when you need to answer questions about current events or the world. Ask specific questions.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Wikipedia\",\n",
    "        func=wikipedia_tool.run,\n",
    "        description=\"Useful for when you need detailed information about a topic. Ask specific questions.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "import time\n",
    "for verse in paste_verses:\n",
    "    time.sleep(1)\n",
    "    custom_prompt = create_custom_prompt(verse)\n",
    "    llm_transformer.prompt = custom_prompt\n",
    "    document = Document(page_content=verse)\n",
    "    document_list = [document]\n",
    "    \n",
    "    graph_docs = llm_transformer.convert_to_graph_documents(document_list)\n",
    "    \n",
    "    try:\n",
    "        for graph_doc in graph_docs:\n",
    "\n",
    "                # Process nodes\n",
    "            for node in graph_doc.nodes:\n",
    "                # Add node to the graph\n",
    "                graph.query(\n",
    "                    f\"MERGE (n:{node.type} {{id: $id}}) \"\n",
    "                    f\"SET n += $properties\",\n",
    "                    {\"id\": node.id, \"properties\": node.properties or {}}\n",
    "                )\n",
    "            \n",
    "            # Process relationships\n",
    "            for rel in graph_doc.relationships:\n",
    "                # Add relationship to the graph\n",
    "                graph.query(\n",
    "                    f\"MATCH (a {{id: $source_id}}), (b {{id: $target_id}}) \"\n",
    "                    f\"MERGE (a)-[r:{rel.type}]->(b) \"\n",
    "                    f\"SET r += $properties\",\n",
    "                    {\n",
    "                        \"source_id\": rel.source.id,\n",
    "                        \"target_id\": rel.target.id,\n",
    "                        \"properties\": rel.properties or {}\n",
    "                    }\n",
    "                )\n",
    "            print(f\"Added graph document to Neo4j: {len(graph_doc.nodes)} nodes, {len(graph_doc.relationships)} relationships\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
